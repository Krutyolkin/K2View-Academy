# commonDB Architecture



## Synchronization Flow

2 types of transactions can be differentiated when updating a commonDB reference table: 
- Short message updates - whereby both update's message and content are both stored on the dedicated Kafka queue.
- Long message updates - whereby the update's message is published on the dedicated Kafka queue while the update's content is stored on Cassandra.

2 different flows occur for each of these cases, depending on whether the update content size exceeds 1000 rows or not. 


### Short Message

The case illustrated below shows how a Synchronisation Job (Sync Job 2) publishes an update notification and short message content on the Kafka Queue dedicated to Table 1, subsequently causing all listening nodes, in the cluster, to write the update directly from Kafka to its SQLite CommonDB copy. 

![image](/articles/22_reference(commonDB)_tables/images/08_commonDB_RefSyncShort.png)



### Long Message

The case illustrated below shows how a Synchronisation Job (Sync Job 1) publishes an update message on the Kafka Queue dedicated to Table T, and how it writes the long message content in Cassandra, subsequently causing any listening node, within the cluster, to write the update content directly from Cassandra to its SQLite CommonDB copy. 

![image](/articles/22_reference(commonDB)_tables/images/09_commonDB_RefSyncLong.png)


## Synchronization Properties

Any transaction involving the common table is done in asynchronous mode, i.e. the updated data cannot be seen until it has been committed, and Fabric updates the relevant commonDB table.

The transaction message is sent to Kafka while its content is saved into Kafka (within the message payload) or in Cassandra, depending on its size.


## Synchronization modes

Regardless of the synchronization type (background or on-demand), Fabric provides 2 different modes to synchronize reference tables data.

### Update Mode: 
This mode is (automatically) selected in cases where row updates to the reference table are needed. 
In this mode, updates are performed in ways of Create/Update/Delete SQL queries directly on the table itself - each node will execute this change locally on his local SQLite commonDB copy) 

- Small updates: - In cases where less than 1000 updates needs to be performed
- Big updates: - In cases exceeding 1000 rows, in which case bulks of 1000 rows are created in Cassandra


### Snapshot Mode:

This mode is (automatically) selected in cases where synchronizing the entire table is needed. 
A full snapshot of the table is then created by an available Fabric node, and is published both to the corresponding Kafka topic dedicated to the table being updated (header only) and to Cassandra (data).

- The header published on the Kafka update topic contains the UUID of the snapshot

- The Cassandra table row contains the following:
  - Uuid â€“ unique per snapshot.
  - Table name
  - message id (from 0 upwards)
  - Data:
  ```
  *list of updates* (as for regular update message), 
  *size of list* (default set using the configuration parameter and which user can changed on a per-snapshot basis). 
  ```

A snapshot will only be published once one of the following actions will be triggered: 

-	The full table synchronization is initialized by a job.
-	Scheduled Sync time has come.
-	Manually, when requested by the user.

Note that in addition, if a delete request is sent to a Reference Table without a ```where``` statement, it is automatically treated as a snapshot update. 






## Snapshots Synchronization Mechanism

### Option 1 - Truncate option is set

- The snapshot is started
- All rows are added to the snapshot
- If any error or exception occurs the rollback process is engaged, otherwise the transaction is committed in a single transaction:
  - Indices are created on the temporary table.
  - The old reference table is dropped.
  - The temporary table is renamed to the Reference Table name.

### Option 2 - Truncate option is not set

- All updates for the Reference Table are executed 
- Once the batch of transactions is available, all the updates are executed in one commit, unless there are failures in which case each update is executed one by one.



## CommonDB Table Initialization
When a new node is  coming up online, and rejoins the current Fabric cluster, all the common tables need to be brought to this node. Two options are available to perform this enrollment:

- **Option 1: Directly from kafka**

The new node connects directly to each kafka topics (one per reference table) to look whether a snapshot is available:
  -	the table is regularly sync-ed as per defined in its sync schedule and therefore a snapshot is available
  -	a snapshot has already been created due to a similar request generated by another node. Note that depending on the size of the table the table will be synced either from the kafka topic or from Cassandra


- **Option 2: *From another node**
  -	no snapshot is available on the corresponding Kafka topic, then the new node asks for a snapshot to be created by another node and waits by listening Kafka
  - another node prepared a snapshot and puts it either in kafka (short) or cassandra (long)



## Use Cases

### Deployment Process

Deploying a new reference table will have the following consequences:
1. All running reference table synchronization jobs will stop.
2  A new table/index will be created on CommonDB 
3. All Kafka Consumer/Topic will be cleared if the table was removed.
4. A new Kafka topic will be created if a new table was added.
5. A new Kafka consumer will be created for each node.
6. New sync jobs will be started (even if deploy failed so not to prevent existing sync-ing although the ref table deploy failed)
7. Existing configuration parameters are applied (such as sync job retry interval) on coordinator node


### Table Removal
This happens as a result of deployment process:

- Table is dropped from CommonDB
-	All local topic consumers and producers are dropped.

### commonDB drop
This happens when running the following command ```drop lutype k2_ref;``` from a Fabric Node:

- All existing tables in common.db are dropped (including the internal and temporary tables)
- All existing sync jobs are stopped.
- All existing consumer / producers are terminated.




## Miscellaneous

- Kafka consumer and producer must be setup along with 2 SSL parameters, one for the consumer topic and one for the producer topic. 

- Operation mode can be with Kafka or in Memory depending on project scope
  - Mode for PoCs w/o kafka dependency
  - no data persistency
  
- Selecting and modifying data contained in Reference Tables can be done by setting the ```common_local_trx``` flag to TRUE, before running a commit.
    ```fabric>set COMMON_LOCAL_TRX=true;```
  
  - Using the [example](/articles/22_reference(commonDB)_tables/02_add_a_reference_table.md#how-do-i-create-a-new-reference-table-in-fabric) defined earlier, and the DEVICESTABLE2017 Reference Table:
  - Enable Reference Table modification: ```fabric>set COMMON_LOCAL_TRX=true;```
  - Use the ```select``` command to view the row that needs modification:
  
    ```
    fabric>select TAC, BRANDMODEL  from DEVICESTABLE2017 where TAC=35156209;
    |TAC     |BRANDMODEL             |
    +--------+-----------------------+
    |35156209|GALAXY J3 2016 SM-J320F|
    ```

  - Start a transaction: ```fabric>begin;```
  - Using the ```update``` command, operate a change in the table: ```fabric>update DEVICESTABLE2017 set BRANDMODEL='GALAXY J3--2016 SM-J320F' where TAC=35156209;```
  - Check that the change was committed:
  
    ```
    fabric>select TAC, BRANDMODEL  from DEVICESTABLE2017 where TAC=35156209;
    |TAC     |BRANDMODEL              |
    +--------+------------------------+
    |35156209|GALAXY J3--2016 SM-J320F|
    ```
  - Close the transaction: ```fabric>end;```
  - Note that if you forget to close the transaction, write sessions to the Reference Table (such as a scheduled Sync) will not work.
