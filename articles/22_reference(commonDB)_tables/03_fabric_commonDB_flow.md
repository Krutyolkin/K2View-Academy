# CommmonDB - Architecture and Flow


## Overview
Fabric CommonDB architecture has been designed to answer the following requirements:

- Keep reference tables in each node
- Manage their synchronization with the source, either automatically in the background or on-demand according to a specific schedule.
- Ensure high availability levels at all times with the lowest access time possible.

## Properties

CommonDB consists of a single SQLite file that contains all the common reference tables that have been designed during their creation process. 
This means that in a distributed environment (Fabric Cluster) each Fabric node contains all common tables within this single file. 
As a rule of thumb, reference tables updates are managed in Kafka, while snapshot updates will be managed via Cassandra.

## Synchronization modes

Fabric provides 2 different modes to synchronize reference tables data.

### Update Mode: 
This mode is (automatically) selected in cases where an update from a reference table is needed. 
In this mode, update are performed in ways of Create/Update/Delete SQL queries directly on the table

- Small updates: - In cases where less than 1000 updates needs to be performed
- Big updates: - In cases exceeding 100 rows, in which case bilks of 1000 rows are created in Cassandrsa


### Snapshot Mode
This mode is (automatically) selected in cases where synchronizing the entire table is needed. A full snapshot of the table is then created by an available Fabric node, and is published both to the corresponding Kafka topic dedicated to the table being updated (header only) and to Cassandra (data).

- The header published on the Kafka update topic contains the UUID of the snapshot

- The Cassandra table row contains the following:
*- Uuid – unique per snapshot.
*- Table name
*- message id (from 0 upwards)
*- Data
 *- *- List of updates (as for regular update message).
 *- *- The size of list (default set using the configuration parameter and which user can changed on a per-snapshot basis). 


A snapshot will only be published once one of the following actions will be triggered: 

o	The full table synchronization is initialized by a job.
o	Scheduled Sync time has come.
o	Manually, when requested by the user.

In addition, if a delete request is sent without a ```where``` statements, is automatically treated as a snapshot update. 



## Synchronization Flow

2 types of transactions can be differentiated Short or long message (kafka vs Cassandra) depending on whether the update exceeds 
### Short Message

[](/articles/22_reference(commonDB)_tables/08_commonDB_RefSyncShort.PNG)


### Long Message

[](/articles/22_reference(commonDB)_tables/09_commonDB_RefSyncLong.PNG)


## Common Area table initialization
When a new node is  coming up online, and rejoins the current Fabric cluster, all the common tables need to be brought to this node. Two options are available to perform this enrollment:

- Option 1: Directly from kafka
The new node connects directly to each kafka topics (one per reference table) to look whether a snapshot is available:
*-	the table is regularly sync-ed as per defined in its sync schedule and therefore a snapshot is available
*-	a snapshot has already been created due to a similar request generated by another node. Note that depending on the size of the table the table will be synced either from the kafka topic or from Cassandra

- Option 2: From another node
*-	no snapshot is available on the corresponding Kafka topic, then the new node asks for a snapshot to be created by another node and waits by listening Kafka
*-	another node prepared a snapshot and puts it either in kafka (short) or cassandra (long)
o	manual command ? maybe new feature
o	startup/recovery process 

## Use Cases
Studio
Existing Reference list used by LU has only meaning on first / force sync (see Advanced features).
-	First time ever Reference Table for node, get of LUI will wait first for Ref Table to re-sync -> if reference needed
-	Sync from reference and then GET – ref_sync and then GET in same session for cases where I need most updates ref table

## Deploy process
1.	Stop all running table sync jobs.
2.	Create table / index on 'Common' 
3.	Clear Kafka Consumer / Topic if table was removed.
4.	Create a Kafka topic (if configured Kafka mode)
5.	Create a Kafka consumer for each node.
Start sync jobs (even if deploy failed) so not to prevent existing sync-ing although the ref table deploy failed
Apply existing config parameters (Sync job retry interval; ) on coordinator server (one addressed by deploy) on sync jobs

## Sync
If truncate applied 
•	Start snap
•	Add all rows to the snap
•	If any error or exception rollback 
•	Else commit – in a single transaction:
o	Create indices on the temporary table.
o	Drop an old reference table.
o	Rename the temporary table.
If no truncate defined 
•	Run updates for table (note, no transactions are available at this stage)
•	batch of transactions is available -> everything in one commit, unless failures (1 by 1)

## Configuration

Config.ini
•	Uses Cassandra for snapshot storing; same Cassandra definition; separate keyspace for long messages
•	Kafka consumer and producer setting - two sections. + 2 SSL parameters sections for consumer and producer 
•	Reference commonDB sqlite files location 
•	Operation mode one of (Kafka , Memory )
•	Mode for PoCs w/o kafka dependency (no data persistency) (achi)

